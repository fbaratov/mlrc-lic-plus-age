{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXy387KJpPu5"
      },
      "outputs": [],
      "source": [
        "#Configure git user name and user e-mail\n",
        "!git config global --user.name \"root-goksenin\"\n",
        "!git config global --user.emil \"goksenin.yuksel@outlook.com\"\n",
        "#Clone the repo into /content files\n",
        "!git clone https://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@github.com/fbaratov/fact-group21.git\n",
        "# Change the directory to lick-caption-bias\n",
        "%cd fact-group21"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jupyterlab\n",
        "!pip install ipywidgets\n",
        "!pip install bertviz"
      ],
      "metadata": {
        "id": "2aKIFFYspUR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "from transformers import PYTORCH_PRETRAINED_BERT_CACHE\n",
        "from transformers import BertConfig, WEIGHTS_NAME, CONFIG_NAME\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import BertModel\n",
        "from transformers import BertPreTrainedModel\n",
        "from bertviz import head_view\n",
        "import torch\n",
        "# import age variables from utils\n",
        "from age_utils import (\n",
        "  young_words,\n",
        "  old_words,\n",
        "  age_words\n",
        ")\n",
        "# import age functiond from utils\n",
        "from age_utils import (\n",
        "  gender_pickle_generator,\n",
        "  race_pickle_generator,\n",
        "  label_human_caption,\n",
        "  label_human_annotations,\n",
        "  match_labels,\n",
        "  make_train_test_split,\n",
        "\n",
        ")\n",
        "from age_dataset import BERT_ANN_leak_data, BERT_MODEL_leak_data\n",
        "from collections import namedtuple\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "tdETlxi1pavJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_bert_model(file_name, model):\n",
        "  '''\n",
        "  Load the model from the weight file.\n",
        "  Please note that the weights will also contain the classification head.\n",
        "  So initialize the model accordingly.\n",
        "  Arguments\n",
        "  ---------\n",
        "  file_name : str\n",
        "      path to the weights\n",
        "  model : torch.nn.Module\n",
        "      BERT Model\n",
        "  '''\n",
        "  model.load_state_dict(torch.load(file_name))\n",
        "\n",
        "def visualize_attention(model, data, tokenizer):\n",
        "  '''\n",
        "  Visualize the attention from the bert model.\n",
        "  Arguments\n",
        "  ---------\n",
        "  model : torch.nn.Module\n",
        "      BERT Model\n",
        "  data : torch.nn.Dataset\n",
        "      Annotation data from authors paper\n",
        "  tokenizer : BertTokenizer\n",
        "      Uncased bert tokenizer for input parsing\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  IPYWidget that displays attention matrix.\n",
        "  \n",
        "  '''\n",
        "  # Get outputs from the dataloader.\n",
        "  input_ids, attention_mask, token_type_ids, age_target, img_id = data[0]\n",
        "  # Convert everything to gpu.\n",
        "  input_ids = torch.unsqueeze(input_ids.cuda(), dim = 0)\n",
        "  attention_mask = torch.unsqueeze(attention_mask.cuda(), dim = 0)\n",
        "  token_type_ids = torch.unsqueeze(token_type_ids.cuda(), dim = 0)\n",
        "  age_target = torch.squeeze(age_target).cuda()\n",
        "  # Pass the arguments to model\n",
        "  outputs = model(input_ids, \n",
        "                  attention_mask=attention_mask, \n",
        "                  token_type_ids=token_type_ids)\n",
        "  # Get the attention matrices.\n",
        "  attention = outputs[-1]    \n",
        "  # Convert tokens into words so it is readable\n",
        "  tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "  # Find the index of [SEP] token. This indicates that sentence has finished.\n",
        "  first_sep = list(tokens).index('[SEP]') \n",
        "  # Get rid of padding attentions and tokens.\n",
        "  new_attention = torch.zeros((len(attention), 1, len(attention), first_sep + 1, first_sep + 1))\n",
        "  for id,att in enumerate(attention):\n",
        "    new_attention[id] = att[:, :,  :first_sep + 1, :first_sep + 1]\n",
        "  # Return the widget.\n",
        "  return head_view(new_attention, tokens[:first_sep + 1], html_action = 'return')  # Display model view"
      ],
      "metadata": {
        "id": "FUFEgZsipVGa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Emulete fake args.\n",
        "ARG = namedtuple('args', ['batch_size', 'workers','test_ratio','task', 'align_vocab','max_seq_length', 'cap_model'])\n",
        "args = ARG(64,1,0.1, 'captioning', True, 64, 'nic')\n",
        "\n",
        "# Load the bert model and tokenizer\n",
        "lang_model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True).cuda()\n",
        "# file_name = ...\n",
        "# load_bert_model(file_name, model)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define the human annotation entries using age_utils.\n",
        "age_val_obj_cap_entries = label_human_annotations(gender_pickle_generator('human'),young_words,old_words) # Human captions\n",
        "\n",
        "d_train, d_test = make_train_test_split(args, age_val_obj_cap_entries)\n",
        "\n",
        "# Define the dataset from authors code\n",
        "trainANNCAPobject = BERT_ANN_leak_data(d_train, d_test, args, age_val_obj_cap_entries, age_words, tokenizer,\n",
        "                                                args.max_seq_length, split='train', caption_ind=0)\n",
        "testANNCAPobject = BERT_ANN_leak_data(d_train, d_test, args, age_val_obj_cap_entries, age_words, tokenizer,\n",
        "                                                args.max_seq_length, split='test', caption_ind=0)\n"
      ],
      "metadata": {
        "id": "7nqFyEHBphkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "html = visualize_attention(lang_model,trainANNCAPobject,tokenizer)\n",
        "with open(\"head_view.html\", 'w') as file:\n",
        "    file.write(html.data)"
      ],
      "metadata": {
        "id": "8VyOIPJipmYz"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}